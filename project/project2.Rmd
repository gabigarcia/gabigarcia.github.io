---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "SDS348"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

##Gabriela Garcia GG27447

# Modeling

## Instructions

A knitted R Markdown document (preferably HTML) and the raw R Markdown file (as .Rmd) should both be submitted to Canvas by 11:59pm on the due date. These two documents will be graded jointly, so they must be consistent (i.e., donâ€™t change the R Markdown file without also updating the knitted document). Knit an html copy too, for later! In the .Rmd file for Project 2, you can copy the first code-chunk into your project .Rmd file to get better formatting. Notice that you can adjust the opts_chunk$set(...) above to set certain parameters if necessary to make the knitting cleaner (you can globally set the size of all plots, etc). You can copy the set-up chunk in Project2.Rmd: I have gone ahead and set a few for you (such as disabling warnings and package-loading messges when knitting)! 

Like before, I envision your written text forming something of a narrative structure around your code/output. All results presented must have corresponding code. Any answers/results/plots etc. given without the corresponding R code that generated the result will not be graded. Furthermore, all code contained in our project document should work properly. Please do not include any extraneous code or code which produces error messages. (Code which produces warnings is fine as long as you understand what the warnings mean.)

## Find data:

Find one dataset with at least 5 variables (ideally more!) that you want to use to build models/test hypotheses. At least one should be categorical (with 2-5 groups, ideally; definitely fewer than 10) and at least two should be numeric (taking on more than 10 distinct values). Ideally, at least of your variables will be binary (if not, you will have to create one by discretizing a numeric or collapsing levels of a categorical). You will need a minimum of 40 observations (*at least* 10 observations for every explanatory variable you have, ideally 20+ observations/variable).

It is perfectly fine to use either dataset (or the merged dataset, or a subset of your variables) from Project 1. However, I might encourage you to diversify things a bit and choose a different dataset to work with (particularly if the variables did not reveal interesting associations in Project 1 that you want to follow up with). The only requirement/restriction is that you may not use data from any examples we have done in class or lab. It would be a good idea to pick more cohesive data this time around (i.e., variables that you actually thing might have a relationship you would want to test). Think more along the lines of your Biostats project.

Again, you can use data from anywhere you want (see bottom for resources)! If you want a quick way to see whether a built-in (R) dataset has binary and/or character (i.e., categorical) variables, check out this list: https://vincentarelbundock.github.io/Rdatasets/datasets.html.


## Guidelines and Rubric

- **0. (5 pts)** Introduce your dataset and each of your variables (or just your main variables if you have lots) in a paragraph. What are they measuring? How many observations?

```{r}
library(fivethirtyeight)
bechdel
bdel<-bechdel%>%filter(year %in% c("2004","2005","2006","2007","2008","2009","2010","2011","2012","2013"))%>%select(-imdb,-test,-period_code,-decade_code)%>% mutate(pass=ifelse(binary=="PASS",1,0))%>%na.omit
bdel


```
*My dataset is the bechdel test for movies from 2004-2013 and the main variables are the if it passed, budget, domestic gross sales when it came out,and the year it came out.the domestic gross sales and budget in the year 2013 are also variables but they will not be used in this assessment. There are a total of 992 observations after removing NAs.*

- **1. (15 pts)** Perform a MANOVA testing whether any of your numeric variables (or a subset of them, if including them all is unreasonable or doesn't make sense) show a mean difference across levels of one of your categorical variables (3). If they do, perform univariate ANOVAs to find response(s) showing a mean difference across groups (3), and perform post-hoc t tests to find which groups differ (3). Discuss the number of tests you have performed, calculate the probability of at least one type I error (if unadjusted), and adjust the significance level accordingly (bonferroni correction) before discussing significant differences (3). Briefly discuss MANOVA assumptions and whether or not they are likely to have been met (no need for anything too in-depth) (2).
```{r}
#manova
summary(manova(cbind(domgross,budget)~clean_test,data=bdel))
#uni anovas for sig
summary(aov(domgross~clean_test,data=bdel)) 
summary(aov(budget~clean_test,data=bdel)) 

#post hoc t
pairwise.t.test(bdel$budget,bdel$clean_test, p.adj="none")

# number of tests done
4
# cal prob of type I error 
(1-.95^4) 
#if unadjusted an use bon correction
 (.05/4)

# manova assumptions 
library(rstatix)
group <- bdel$clean_test 
DVs <- bdel %>% select(domgross,budget)
#Test multivariate normality for each group (null: assumption met)
sapply(split(DVs,group), mshapiro_test)


```
*In the MANOVA for domostic gross profit and budget there was a difference in mean across the test result. After running ANOVAs for each variable there was a significant difference for budget across the bechdal test results. Using a post hoc t test there was signicant difference on the budget for test results between passing and when women do not speak, talk about men, and those that do not quite pass the test. With 4 tests the predicted Type I error rate is 0.185 and when the corrected p-value is .0125 the only significant difference seems to be between movies with passing scores and movies where women do not speak. The assumption of multivariate normality was violated in this dataset. It is not random since it is the top grossing films in America and since it fails normality the covarineces are not equal.*

- **2. (10 pts)** Perform some kind of randomization test on your data (that makes sense). The statistic can be anything you want (mean difference, correlation, F-statistic/ANOVA, chi-squared), etc. State null and alternative hypotheses, perform the test, and interpret the results (7). Create a plot visualizing the null distribution and the test statistic (3).

```{r}
#randomization test that makes sense
thettest<-t.test(data=bdel,budget~pass)
thettest
#plot null dist and test stat
N=20 
samp<-rnorm(N)
nullttest<-t.test(samp)
tcrit<-qt(0.025, df=(N-1))

dummm<-seq(-6, 6, length=10^4)#For the plot

plot(dummm, dt(dummm, df=(N-1)), type='l', xlab='t', ylab='y')+ abline(v=thettest$statistic, col='blue' )+ abline(v=tcrit, col='red',lty=3)+ abline(v=-tcrit, col='red',lty=3)
```
*The null hypothesis is that there is no diffence in the mean budget of movies that pass compared to movies that fail, the alternative is that there will be a diffence between passing and failing movies. From the data there is a significant differnce between the mean budgets where groups that fail the bechdal test have a higher budget than those that pass.*

- **3. (35 pts)** Build a linear regression model predicting one of your response variables from at least 2 other variables, including their interaction. Mean-center any numeric variables involved in the interaction.

    - Interpret the coefficient estimates (do not discuss significance) (10)
    - Plot the regression using `ggplot()` using geom_smooth(method="lm"). If your interaction is numeric by numeric, refer to code in the slides to make the plot or check out the `interactions` package, which makes this easier. If you have 3 or more predictors, just chose two of them to plot for convenience. (8)
    - Check assumptions of linearity, normality, and homoskedasticity either graphically or using a hypothesis test (4)
    - Regardless, recompute regression results with robust standard errors via `coeftest(..., vcov=vcovHC(...))`. Discuss significance of results, including any changes from before/after robust SEs if applicable. (8)
    - What proportion of the variation in the outcome does your model explain? (4)

```{r}
library(lmtest)
library(sandwich)

#linear regression to predict rsp with two other variables 
p2lr<-lm(year ~budget + domgross , data=bdel)
summary(p2lr)
bdel<-bdel%>% mutate(c.dg=domgross-mean(domgross), c.bd=budget-mean(budget))
p2lri<-lm(year ~c.bd *c.dg , data=bdel)
summary(p2lri)

#plot 
bdel%>% ggplot(aes(y=c.dg,x=c.bd, color=year)) + geom_point() + geom_smooth(method = "lm",se=F)

#ass w/out int
p2resids<-p2lr$residuals
p2fitvals<-p2lr$fitted.values
plot(p2fitvals,p2resids); abline(h=0, col='red')

par(mfrow=c(1,2)); hist(p2resids); qqnorm(p2resids); qqline(p2resids, col='red')
ggplot(bdel,aes(y=c.dg,x=c.bd,color=year))+geom_point()

#ass w/ int
p2iresids<-p2lri$residuals
p2ifitvals<-p2lri$fitted.values
plot(p2ifitvals,p2iresids); abline(h=0, col='red')

par(mfrow=c(1,2)); hist(p2iresids); qqnorm(p2iresids); qqline(p2iresids, col='red')


#robust
coeftest(p2lr,vcov = vcovHC(p2lr))
coeftest(p2lri,vcov = vcovHC(p2lri))

#variance explained

```
*The coeffient for budget means that the prediction for year is 6.050e-09 more likely with each increase and for gross dometic profits it is 2.8584e-10 less likely to predict year with each increase in profits. With intraction it changes to 6.346e-09 more chance each increase in budget,2.988e-10 more for increase in profits, and  -8.142e-18 decrease when accounting for both.Only the budget seems to predict the year for each increase both when not counting for interaction and when it is. When adjusting for robust SE the coeffients stay the same with the p-values increasing slightly but no significant change except int the interaction between budget and profits which decreased by almost half, but is still insignificant. This data fails the assumptions for normality, linarity, and homoskedacity.The model without interaction explains  0.01161 of variance, and with interaction explains  0.0112 of variance.*

- **4. (5 pts)** Rerun same regression model (with the interaction), but this time compute bootstrapped standard errors (either by resampling observations or residuals). Discuss any changes you observe in SEs and p-values using these SEs compared to the original SEs and the robust SEs)

```{r}
#rerun interatction model with bootstrapped errors
samp_p2<-replicate(5000, {
  boot_p2 <- sample_frac(bdel, replace=T) 
  fitp2 <- lm(year ~c.bd *c.dg, data = boot_p2) 
  coef(fitp2) 
}) 
samp_p2 %>% t %>% as.data.frame %>%  summarize_all(sd) 


```
*With interaction the bootstrapping SE for budget  2.319303e-09 differs slightly higher than the original SE 2.282e-09 and the rubust SE 2.2983e-09. With interaction the bootstrapping SE for domestic gross profits 1.794651e-09 is higher than the original SE 1.709e-09 and lower than the rubust SE 1.8069e-09. With interaction the bootstrapping SE for budget and profits 1.103638e-17 is higher than the original SE 1.064e-17 and the rubust SE 7.4144e-18.As the SE's increase in the resampled data, the p-values increase as well.*

- **5. (25 pts)** Fit a logistic regression model predicting a binary variable (if you don't have one, make/get one) from at least two explanatory variables (interaction not necessary). 

    - Interpret coefficient estimates in context (10)
    - Report a confusion matrix for your logistic regression (2)
    - Compute and discuss the Accuracy, Sensitivity (TPR), Specificity (TNR), Precision (PPV), and AUC of your model (5)
    - Using ggplot, make a density plot of the log-odds (logit) colored/grouped by your binary outcome variable (3)
    - Generate an ROC curve (plot) and calculate AUC (either manually or with a package); interpret (5)

```{r}
library(plotROC)

p2gr<-glm(pass~budget+domgross, data=bdel, family="binomial")
coef(p2gr)
bdel<-bdel%>%mutate(p2prob=predict(p2gr,type="response"))
bdel<-bdel%>% mutate(p2pred=ifelse(p2prob>.5,"T","F"))

table(truth=bdel$pass, prediction=bdel$p2pred)%>%addmargins#confustion matrix

# accuracy:proportion of all cases that were correctly classified
(313+256)/992
# sensitivity: proportion of true that were correctly classified
256/471 
# specificity: proportion of false cases that were correctly classified
313/521

#roc and auc
swROC<-ggplot(bdel)+geom_roc(aes(d=pass,m=p2prob), n.cuts=0)
swROC
calc_auc(swROC)

#logit density
bdel$logit<-predict(p2gr)
bdel%>%ggplot(aes(logit, fill=binary))+geom_density(alpha=.3)+
  geom_vline(xintercept=0,lty=2)

```
*The coeffients mean that the probabililty of passing decreases by 8.750348e-09 as budget increases and increases by 2.563396e-09 as profits increase.Accuracy was 0.574, Sensitivity was 0.544, Specificity was 0.601, and AUC was 0.603 which means that it is not a good model to predict if a movie will pass the bechdal test. The ROC curve makes it seem that the AUC would be low and the AUC value confirms that it is a poor predictor for passing.*

- **6. (25 pts)** Perform a logistic regression predicting the same binary response variable from *ALL* of the rest of your variables (the more, the better!) 

    - Fit model, compute in-sample classification diagnostics (Accuracy, Sensitivity, Specificity, Precision, AUC), and interpret (5)
    - Perform 10-fold (or repeated random sub-sampling) CV with the same model and report average out-of-sample classification diagnostics (Accuracy, Sensitivity, Specificity, Precision, and AUC); interpret AUC and compare with the in-sample metrics (10)
    - Perform LASSO on the same model/variables. Choose lambda to give the simplest model whose accuracy is near that of the best (i.e., `lambda.1se`). Discuss which variables are retained. (5)
    - Perform 10-fold CV using only the variables lasso selected: compare model's out-of-sample AUC to that of your logistic regressions above (5)


```{r}
#the function
class_diag<-function(probs,truth){
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]
  f1=2*(sens*ppv)/(sens+ppv)

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE){
    truth<-as.numeric(truth)-1}
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,f1,auc)
}
#big lm
bdelaa<-bdel%>% select(year,pass, budget,domgross)
p2model<-glm(pass~(.), data=bdelaa, family="binomial")

prob<-predict(p2model,type="response")
class_diag(prob,bdelaa$pass)

#table(predict=as.numeric(prob>.5),truth=bdel$pass)

#10 fold
set.seed(1234)
k=10
bdela <- bdel %>% select(year,pass, budget,domgross)%>% sample_frac #put rows of dataset in random order
folds <- ntile(1:nrow(bdela),n=10) #create fold labels

diags<-NULL
for(i in 1:k){
  train <- bdela[folds!=i,] #create training set (all but fold i)
  test <- bdela[folds==i,] #create test set (just fold i)
  truth <- test$pass #save truth labels from fold i
  
  fit <- glm(pass~(.), data=train, family="binomial")
  probs <- predict(fit, newdata=test, type="response")
  
  diags<-rbind(diags,class_diag(probs,truth))
}

summarize_all(diags,mean)

#lasso
library(glmnet)
set.seed(1234)
k=10
y<-as.matrix(bdel$pass) #grab response
bdel_preds<-model.matrix(pass~(.), data=bdelaa, family="binomial")[,-1] #predictors (drop intercept)

cv <- cv.glmnet(bdel_preds,y, family="binomial") #picks an optimal value for lambda through 10-fold CV

cv<-cv.glmnet(bdel_preds,y,family="binomial")
lasso_fit<-glmnet(bdel_preds,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso_fit)

bdelprob<- predict(lasso_fit, bdel_preds, type="response")
bdelprob

#better 10 fold
bdelaaa<- bdela%>% select(pass, budget)%>% sample_frac
folds <- ntile(1:nrow(bdelaaa),n=10) #create fold labels

diags<-NULL
for(i in 1:k){
  train <- bdelaaa[folds!=i,] #create training set (all but fold i)
  test <- bdelaaa[folds==i,] #create test set (just fold i)
  truth <- test$pass #save truth labels from fold i
  
  fit <- glm(pass~budget, data=train, family="binomial")
  probs <- predict(fit, newdata=test, type="response")
  
  diags<-rbind(diags,class_diag(probs,truth))
}

summarize_all(diags,mean)
```
 *I selected the unique variables and kept year, domgross, and budget and excluded what I built my dummy variable from and the 2013 adjusted domgross and budget. The AUC for the logistic regression is 0.608 which means it is poor at distingushing if the movie will pass the bechdal test. Without the lasso the 10 fold AUC is 0.598 which indicates it is slightly worse at distinguing a passing or failing status. The only retained variable after the lasso was budget but the AUC decreased slightly and indicates the model is also bad even after adjusting. The out of sample AUC 0.593 was slightly lower than the logicistic regression 0.608. which means that while both are bad, the out of sample model is worse at distinguishing a passing or failing status.*

...





